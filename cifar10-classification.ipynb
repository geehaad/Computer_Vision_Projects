{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gehadabdelghany/cifar10-classification?scriptVersionId=115517370\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Cifar10 classification","metadata":{}},{"cell_type":"markdown","source":"The purpose of this assignment is to learn:\n- The basics of data loading and preparation\n- Classification using the k-NN algorithm\n- Classification using a Support Vector Machine (SVM)\n- Calculation of metrics to assess the performance of your model","metadata":{"id":"mzA42PLsGD-E"}},{"cell_type":"markdown","source":"## Part 0. Dataset Preparation\nDownload the Cifar10 dataset and devise an appropriate training set split.\nNote that some sources e.g. sklearn have reduced the samples to only 1797 (this could negatively affect your results).\n\nIn this section:\n<ul>\n<li>Download the dataset as described above</li>\n<li>Divide the raw data into appropriate training and test sets for both the images and corresponding labels for use with the subsequent parts of the assignment.</li>\n<li>Visualize five samples of each class by plotting a grid using the matplotlib library.</li>\n</ul>\n","metadata":{"id":"SYbojDtvGD-F"}},{"cell_type":"code","source":"# !pip install tensorflow","metadata":{"id":"vEXROwmR5G8U","outputId":"1c3feb7c-952e-4f84-a3cf-4889684d1a8d","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import libraries\nimport tensorflow as tf\nfrom tensorflow.keras import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"id":"SImbtN5N0vtX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the dataset as train, test features and labels\n(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()","metadata":{"id":"VMrqAiA0GD-F","outputId":"4ca89e60-cd01-4b7a-d115-3f959409396a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"id":"XHgSL70J3fKn","outputId":"2edd7be5-22f4-45cc-f79f-7555f8d040b7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"id":"cJf0mlC1e4f5","outputId":"ddebfd0e-00fe-48a4-d051-00aa12c69191"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"id":"OP_MGW413fq0","outputId":"d69d3ebc-2118-4718-c1ae-cbaca1fea43e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.shape","metadata":{"id":"g-JGMzHBe855","outputId":"20869c00-d73e-4da4-8ea7-39da1540c679"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing 5 samples of the dataset","metadata":{"id":"wq6qGM1IvEXr"}},{"cell_type":"code","source":"# Print Grid of all calsses\n'''\nLabel Classes: \n                'Plane',\n                'Car',\n                'Bird',\n                'Cat',\n                'Deer',\n                'Dog',\n                'Frog',\n                'Horse',\n                'Ship',\n                'Truck'\n'''\n\nclasses = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\nnum_classes = len(classes)\nsamples_per_class = 5\n\nfor j, cls in enumerate(classes):\n    idxs = np.flatnonzero(y_train == j)\n    idxs = np.random.choice(idxs, samples_per_class)\n    for i, idx in enumerate(idxs):\n        plt_idx = i * num_classes + j + 1\n        plt.subplot(samples_per_class, num_classes, plt_idx)\n        plt.imshow(X_train[idx])\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls)\nplt.show()","metadata":{"id":"n5bevIOcWm5x","outputId":"67e1d410-60f8-438c-b1c4-b889f1d371c7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sampling the dataset with the same number of the values in each class","metadata":{"id":"HXtaR72ggsxh"}},{"cell_type":"code","source":"X_train_s, _, y_train_s, _ = train_test_split(\n    X_train, y_train, stratify=y_train,train_size=0.2, random_state=42)\n\nX_test_s, _, y_test_s, _ = train_test_split(\n    X_test, y_test, stratify=y_test,train_size=0.2, random_state=42)","metadata":{"id":"5jAVTl60gsDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_s.shape","metadata":{"id":"ViYDEpkVg36b","outputId":"e89a7b97-e50a-46d5-8344-2c0cacf07d08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_s.shape","metadata":{"id":"tt9u8_Ldg30N","outputId":"910cbc7a-1f8b-4d18-9af5-c601c80758a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for number of unique classes in each class to ensure that the data is balanced\nnp.unique(y_train_s, return_counts=True)","metadata":{"id":"Rh0nwnlEi-Co","outputId":"0d875b58-4062-44a1-fdc8-d3684e3b7fcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_s.shape","metadata":{"id":"N68IJXUBg3x1","outputId":"019555b5-f251-4de8-a895-895b70e413ac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_s.shape","metadata":{"id":"m8U8Ul1Kg3k5","outputId":"e78cdfab-cf11-4106-e47c-2ce5550423c5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(y_test_s, return_counts=True)","metadata":{"id":"AymNzJpDi61b","outputId":"b8556cf7-d674-4695-fc5f-b638cd277eb3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalization","metadata":{"id":"xw-NlommiXg-"}},{"cell_type":"code","source":"# Reduce pixel values\nX_train_s = X_train_s / 255.0\nX_test_s = X_test_s / 255.0","metadata":{"id":"v-DdL2F-OHUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flatten the label values\ny_train_s, y_test_s = y_train_s.flatten(), y_test_s.flatten()","metadata":{"id":"YEdgBjBqx3YW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"h3YWXpZS5G8S"}},{"cell_type":"markdown","source":"* In this part I load the data from Tensorflow.keras.\n* The data contains 60000 samples, and split it into 50000 training, and 10000 testing.\n* As it takes alot to train models using this big data, I sample the data into smapler set, with the same number of values in each class.\n* I normalize the pixel values of images between 0 and 1 by dividing it by 255.\n* Also I flatten the data.","metadata":{"id":"LKJ6mtvX5G8T"}},{"cell_type":"markdown","source":"## Part 1. k-NN Classifier (2 Marks)\nFor this section you will implement a simple kNN Classifier on the Cifar10 dataset. To do this you will need to perform the following steps:\n- Create a k-NN class. Your class must contain a method that returns predictions for your test set.\n- Provide a suitable distance metric that you will use to calculate the nearest neightbours. You may choose the distance metric you believe is most suitable.\n- Calculate the k nearest neighbours and make predictions.\n- Choose the a value for _k_ that results in the highest accuracy on your test set. Show how you found this value.\n\nWhen your classifier is working:\n- Use sklearn to calculate accuracy and plot a confusion matrix using your predictions.\n- Provide a brief discussion of your results\n\n\n","metadata":{"id":"1mjF3FB5GD-G"}},{"cell_type":"code","source":"# Write your KNN class here","metadata":{"id":"fu53BJqOGD-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K Neares Neighbor class that returns predictions for your test set\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n\n    # euclidean distance metric\n    def _get_euclidean_distance(self, x1, x2):\n        # calculate euclidean distance for a row pair\n        sum_squared_distance = np.sum((x1 - x2)**2)\n        return np.sqrt(sum_squared_distance)\n    # Train function\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n    # Prediction\n    def predict(self, X_test):\n        # get predictions for every row in test data\n        y_pred = [self._get_single_prediction(x_test_row) for x_test_row in X_test]\n        return np.array(y_pred)\n\n    def _get_single_prediction(self, x_test_row):\n        # get distances of test_row vs all training rows\n        distances = [self._get_euclidean_distance(x_test_row, x_train_row) \n                     for x_train_row in self.X_train]\n        # get indices of k-nearest neighbors -> k-smallest distances\n        k_idx = np.argsort(distances)[:self.k]\n        # get corresponding y-labels of training data\n        k_labels = [self.y_train[idx] for idx in k_idx]\n        # return most common label\n        return np.argmax(np.bincount(k_labels))\n        \n    # Compute accuracy\n    def accuracy(self,y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\n        return accuracy","metadata":{"id":"o69xoG-3Oxod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 3\nclf = KNN(k=k)\nclf.fit(X_train_s ,y_train_s)\npredictions = clf.predict(X_test_s)","metadata":{"id":"ysCrHaH-Q8Ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = clf.accuracy(y_test_s, predictions)","metadata":{"id":"Q1JR6pcOERij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print accuracy \nprint(\"Accuracy\",score * 100 )","metadata":{"id":"mUUCZcuUDKo8","outputId":"eedc4ab0-f519-4fee-ed17-9fb738cd34ab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choose best K","metadata":{"id":"folVw7ZxyrET"}},{"cell_type":"code","source":"errors = []\nAccurs = [] \n\nfor k in [1,3,5,7,9,13,17,23]:\n    Knn = KNN(k=k)\n    Knn.fit(X_train_s ,y_train_s)\n    prediction = Knn.predict(X_test_s)\n    accr = Knn.accuracy(y_test_s, prediction)\n    Accurs.append(accr)\n    error = np.mean(prediction != y_test_s)\n    errors.append(error)\n    print('At K= ', k)\n    print(\"Accuracy =\", accr)\n    print(\"Error =\", error)\n    print(\"#################################################\")\n ","metadata":{"id":"G76D6VgVQvpd","outputId":"a72e68ab-7808-4d0f-f243-12402529b85b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot error vs. K values","metadata":{"id":"-F0z-dX1OTVI"}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nplt.plot([1,3,5,7,9,13,17,23],errors,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\n\nplt.title('Error vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error')\nprint(\"Minimum Error:-\",min(errors),\"at K =\",errors.index(min(errors))+1)   ","metadata":{"id":"RN7-SkIBOR2J","outputId":"35111e91-357f-4bb2-ed79-fd538ed361f1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Acuracy vs. K values","metadata":{"id":"jh2FAooJOYEo"}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nk =[1,3,5,7,9,13,17,23]\nplt.plot([1,3,5,7,9,13,17,23],Accurs,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\n\nplt.title('Accuracy vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nprint(\"Maximum Accuracy:-\",max(Accurs),\"at K =\",k[Accurs.index(max(Accurs))])  ","metadata":{"id":"sgtxot5pPSrC","outputId":"e3506a01-58b6-4ad6-ba3d-ed8f5e3a62d3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* I used accuracy and error to help me choice the best K value, and both said the same thiang, that K is the best value.","metadata":{"id":"ONhAJoMg5G8u"}},{"cell_type":"markdown","source":"## Train KNN at best K","metadata":{"id":"VPIkku1Z5G8v"}},{"cell_type":"code","source":"Knn = KNN(k=7)\nKnn.fit(X_train_s ,y_train_s)\nprediction = Knn.predict(X_test_s)\naccr = Knn.accuracy(y_test_s, prediction)","metadata":{"id":"hDzCY7hY5G8v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix","metadata":{"id":"5rPYAHMR5G8w"}},{"cell_type":"code","source":"confusion_matrix = metrics.confusion_matrix(y_test_s, prediction)\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(y_test_s))\n\ncm_display.plot()\nplt.title('Confusion Matrix for cifar10 at K=7')\nplt.show()","metadata":{"id":"TAaoKDCN5G8w","outputId":"757e1451-1d74-48c5-d419-e63548be6c1c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have 200 value for each class, The confuison matrix says that the model is able to predict the class eight better than other classes as it manages to predict 138 images correctly, and managed to predict 105 imagess of 0 class colrecttly.\n* But the other classes correct predict was less than the half of the total values in a single class.\n* The class that the model was confused with the most is class 3, as it only manages to predict 21 images.","metadata":{"id":"NzaKgcGBMKD5"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"MQFqxkVxjJz2"}},{"cell_type":"markdown","source":"* In this part I create a Knn function that take k and calculate the nearest k neighbours using euclidean distance, this distance is the most widely used distance metric and it is the default metric in SKlearn library.\n* I used a random K = 3, and calculate score, = 26.15, and predictions.\n* Then I used different values of K, to find the best value of it, using the accuracy and error rate, and plot both with the number of K and compare the values and found out that when K=7, we have the best accuracy and the less error.\n* Finally, I train my Knn model using k=7 and get the predictions and plot the confusion matrix.","metadata":{"id":"VztgLVS6OZH7"}},{"cell_type":"markdown","source":"## Part 2. Logistic Regression\n\nFor this section you will perform binary classification using logistic regression. Just as in Part 1. you will use the Cifar10 dataset, however to obtain a result for each class using logistic regression you will need to use a One-vs-Rest (OvR) approach to acheive multi-class classification.\n\nUsing ```LogisticRegression()``` in sklearn, write a function to execute the OvR strategy for the Cifar10 classes. Do not use the built-in ```OneVsResClassifier()``` method. You will need to follow these basic steps:\n- Train a binary classifier for each class, where the target class is a \"positive\" results and the combination of the remaining classes are \"negative\". For Cifar10 you will need 10 models.\n- For each test sample compute the probabilities for each model\n- Select the argmax of the probabilities to obtain the predicted class\n\nCollect your predictions from the test set and compute the accuracy score and plot a confusion matrix.","metadata":{"id":"AkCOKxnZGD-I"}},{"cell_type":"code","source":"# Write your logistic regression code here.\nfrom sklearn.linear_model import LogisticRegression\nfrom numpy import argmax\nfrom numpy import asarray\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics","metadata":{"id":"IZqjENjsGD-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I had an error when running the code with the original data shape, so I have to reshape it.\nnsamples, nx, ny, nz = X_train_s.shape\nd2_train_dataset = X_train_s.reshape((nsamples,nx*ny*nz))\nnsampless, nxx, nyy, nzz = X_test_s.shape\nd2_test_dataset = X_test_s.reshape((nsampless,nxx*nyy*nzz))","metadata":{"id":"ikby4cWrXh7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OvR function to calculate the logistic regression for each class vs rest.\ndef OvR(r):\n    y_train_r = [[1] if i == r else [0] for i in y_train_s ]\n    y_test_r = [[1] if i == r else [0] for i in y_test_s ]\n    LR = LogisticRegression(random_state=0, max_iter=1000).fit(d2_train_dataset, np.ravel(y_train_r,order='C'))\n    pre = LR.predict_proba(d2_test_dataset)[:,1]\n    return pre","metadata":{"id":"L_8nPsQlUfMN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 0 VS. All","metadata":{"id":"5eTlj24vSneL"}},{"cell_type":"code","source":"OVR0 = OvR(0)","metadata":{"id":"8dqsOrckTL4N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1 VS. All","metadata":{"id":"JCd7jcGqSpoi"}},{"cell_type":"code","source":"OVR1 = OvR(1)","metadata":{"id":"jYgUf8n1TKa9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2 VS. All\n","metadata":{"id":"SkclMv3ASrHy"}},{"cell_type":"code","source":"OVR2 = OvR(2)","metadata":{"id":"9Uyp-rm3TM47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3 VS. All\n","metadata":{"id":"ioNXiR1cSs-U"}},{"cell_type":"code","source":"OVR3 = OvR(3)","metadata":{"id":"OGbc-B7-TOYn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4 VS. All","metadata":{"id":"Y3VdNT39TDzZ"}},{"cell_type":"code","source":"OVR4 = OvR(4)","metadata":{"id":"8zOfEin9TPHG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5 VS. All","metadata":{"id":"aBJ5jnh4TGS2"}},{"cell_type":"code","source":"OVR5 = OvR(5)","metadata":{"id":"XjD4d_KBTbHJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6 VS. All","metadata":{"id":"I20MjfPZTQwS"}},{"cell_type":"code","source":"OVR6 = OvR(6)","metadata":{"id":"7bIXNWhtTbhS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7 VS. All","metadata":{"id":"cC8-8Op7TSjW"}},{"cell_type":"code","source":"OVR7 = OvR(7)","metadata":{"id":"FRQ7RLrATbyi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8 VS. All","metadata":{"id":"agVSStz2TU3N"}},{"cell_type":"code","source":"OVR8 = OvR(8)","metadata":{"id":"lJ-0qAM_TccB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9 VS. All","metadata":{"id":"r0QARdLkTXG7"}},{"cell_type":"code","source":"OVR9 = OvR(9)","metadata":{"id":"7cIoNMJlC70f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Array of all probabilities\nprobs = asarray([OVR0, OVR1,OVR2, OVR3, OVR4,OVR5,OVR6,OVR7,OVR8,OVR9])\nprobs.shape","metadata":{"id":"5ysvZo31mMgU","outputId":"cf1af236-a23f-4435-8f89-d1517ec27706"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Argmax probabilities","metadata":{"id":"iipg819fl68H"}},{"cell_type":"code","source":"result = argmax(probs, axis=0)","metadata":{"id":"4mNnElPTlzXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The predicted classes\nresult","metadata":{"id":"QdQi9915m32m","outputId":"6e7ed20c-4ee6-4f7c-e4f3-03ff444b11c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The accuracy of argmax\naccuracy_score(y_test_s, result)","metadata":{"id":"Q9Lo9pQNfd7s","outputId":"50dbb9fd-ee67-495c-a4a3-7671a9e361c9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confiuson matrix","metadata":{"id":"e0dyvikagLxo"}},{"cell_type":"code","source":"confusion_matrix = metrics.confusion_matrix(y_test_s, result)\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(y_test_s))\n\ncm_display.plot()\nplt.title('Confusion Matrix for cifar10')\nplt.show()","metadata":{"id":"mzPS22ZUgLmn","outputId":"df310595-c2cb-4f74-8d2c-4b520b0b69fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have 200 value for each class, The confuison matrix says that the model is able to predict the class six better than other classes as it manages to predict 128 images correctly, and managed to predict 113  Imagess of 0 class colrecttly, like the previous model it made a good performance with this class, and is better with class 9 than the last model with 101 corect predictions.\n\n* But the other classes correct predict was less than the half of the total values in a single class.\n\n* The classes that the model was confused with the most is class 2, as it only manages to predict 4 images.","metadata":{"id":"sG_Z2CdqWF_C"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"IEpeiHYOne8i"}},{"cell_type":"markdown","source":"* In this part I use Logistic Regression model from Sklearn library, and as logistic regression is binary classification and our problem is muliclass classification, we need to use OvR.\n* In OvR part, I made the label of the class I want to predict = 1 and other classses = 0, and use Logistic regression to predict the output and calculate the probabilities, and finally we have 10 models.\n* Then I get the argmax of these 10 models, and compare its predictions with the true predictions and plot the confusion matrix.\n\n","metadata":{"id":"rlsA61NjnjBH"}},{"cell_type":"markdown","source":"## Part 3. Support Vector Machine (SVM) Classification\n\nIn Part 3. you will use Scikit-learn to perform classification, again on the Cifar10 dataset. You can use the built in SVM library for classification. As with logistic regression, SVM is designed for binary classification. However, in this case Scikit-learn will handle the OvR models behind the scenes.\n\nYour task is to compare different modes of the SVM and determine the best performer. \n\nCreate an SVM baseline using the <code>LinearSVC()</code> function. Make sure to use the primal solution and use \"ovr\" for multiclass Calculate the accuracy score for comparison.\n\nNext you will explore the effect of the cost parameter on the accuracy.\n    <ul>\n    <li>Run the classification with a range of C values For example: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000 ]</li>\n    <li>Plot the results as an accuracy vs. C-parameter curve on a logarithmic scale.</li>\n    </ul>\n\nAdd a regularization term.\n    <ul>\n    <li>Rerun the above experiment, but this time use L1 regularization.</li>\n    <li>Again, plot the results as an accuracy vs. C-parameter curve on a logarithmic scale.</li>\n    </ul>\n\nFor the final experiment you will use the ```SVC()``` function to run the classifer with a kernel.\n    <ul>\n    <li>Use a radial basis function when training a new model</li>\n    <li>Find the optimal combination of values for the cost and gamma parameters. Use the following values in your loop:<br/>\n        <div style=\"margin-left:40px\"><code>\n        for cost in [0.01, 0.1, 1, 10, 100]:<br/>\n        &emsp;for gamma in [0.01, 0.1, 1, 10, 100]:\n        </code></div>\n    </li>\n    <li>Again, plot the results as an accuracy vs. C-parameter curve on a logarithmic scale.</li>\n    </ul>\n\n\nChoose the model with the highest accuracy and plot the confusion matrix. In your discussion explain the results of your experiments and the reason for increased performance from the baseline (if any). Comment on the effect of the cost-paramenter and the L1 penalty on accuracy as well as any overfitting you observed. Discuss the confusion matrix of the model accuracy and provide some reasons for high-values found off the main diagonal.\n\n","metadata":{"id":"J9VRU5gtGD-J"}},{"cell_type":"code","source":"# Run your SVM experiments here.\nfrom sklearn import svm","metadata":{"id":"VxlWoTg7GD-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM Baseline with LinearSVC","metadata":{"id":"VHMe6wFL5G89"}},{"cell_type":"code","source":"base_clf = svm.LinearSVC(multi_class='ovr', dual = False)\nbase_clf.fit(d2_train_dataset, np.ravel(y_train_s,order='C'))\npre_svm = base_clf.predict(d2_test_dataset)\naccr_svm = base_clf.score(d2_test_dataset, np.ravel(y_test_s,order='C'))","metadata":{"id":"neMbKpYS5G8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr_svm","metadata":{"id":"rtyNiT9-HSYf","outputId":"e71397c6-b628-4097-9235-3efbe998a954"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Different cost patameter values","metadata":{"id":"Rky46aXE5G8-"}},{"cell_type":"code","source":"Accurs_svm = []\nCs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000 ]\nfor c in Cs:\n    lin_clf = svm.LinearSVC(multi_class='ovr', dual = False, C = c)\n    lin_clf.fit(d2_train_dataset, np.ravel(y_train_s,order='C'))\n    pre_svm = lin_clf.predict(d2_test_dataset)\n    accr_svm = lin_clf.score(d2_test_dataset, np.ravel(y_test_s,order='C'))\n    Accurs_svm.append(accr_svm)\n    print('At C= ', c)\n    print(\"Accuracy =\", accr_svm)","metadata":{"id":"0gRuAEC4OZgR","outputId":"1de665ca-5c14-4f5d-b186-bda99d533f62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracies at different Cs.\nAccurs_svm","metadata":{"id":"UyIcddBeqmbc","outputId":"45497751-a9d6-4790-8768-58f65e5a040c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracies vs C values plot.\n\nplt.figure(figsize=(10,6))\nplt.plot(Cs,Accurs_svm,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Accuracy vs. C Value')\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.xscale('log')\n\nprint(\"Maximum Accuracy:-\",max(Accurs_svm),\"at C =\",Cs[Accurs_svm.index(max(Accurs_svm))])","metadata":{"id":"sgJHgdOAl7PJ","outputId":"52e74b24-de45-42f5-ce1b-89c3dbf606fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## L1 regularization","metadata":{"id":"9LD-7rfLeqiF"}},{"cell_type":"code","source":"Accurs_svm_reg = []\nCs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nfor c in Cs:\n    lin_clf = svm.LinearSVC(multi_class='ovr', dual = False, penalty='l1', C = c)\n    lin_clf.fit(d2_train_dataset, np.ravel(y_train_s,order='C'))\n    pre_svm_reg = lin_clf.predict(d2_test_dataset)\n    accr_svm_reg = lin_clf.score(d2_test_dataset, np.ravel(y_test_s,order='C'))\n    Accurs_svm_reg.append(accr_svm_reg)\n    print('At C= ', c)\n    print(\"Accuracy =\", accr_svm_reg)\n","metadata":{"id":"vuiGZ9ycm2fD","outputId":"db8654e0-a168-42e8-c5d1-4060e26370e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nplt.plot(Cs,Accurs_svm_reg,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\n\nplt.title('Accuracy vs. C Value')\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.xscale('log')\n\nprint(\"Maximum Accuracy:-\",max(Accurs_svm_reg),\"at C =\",Cs[Accurs_svm_reg.index(max(Accurs_svm_reg))])","metadata":{"id":"GBdwGGZCtOAx","outputId":"0a3dfad2-2f77-4703-c4c4-0e04b49b49e2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cost and gamma different values","metadata":{"id":"A73DB19CeyY9"}},{"cell_type":"code","source":"from sklearn.svm import SVC","metadata":{"id":"zfjlYfg01w6j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nAccurs_svm_gC = []\nfor cost in [0.01, 0.1, 1, 10, 100]:\n  for gamma in [0.01, 0.1, 1, 10, 100]:\n      lin_clf = SVC(gamma=gamma, C = cost)\n      lin_clf.fit(d2_train_dataset, np.ravel(y_train_s,order='C'))\n      pre_svm = lin_clf.predict(d2_test_dataset)\n      accr_svm = lin_clf.score(d2_test_dataset, np.ravel(y_test_s,order='C'))\n      Accurs_svm_gC.append(accr_svm)\n      print('At C= ', cost, 'and At gamma =', gamma)\n      print(\"Accuracy =\", accr_svm)","metadata":{"id":"Nqul2sPqzO5k","outputId":"b7c811f5-e14e-48db-c988-5cf21ad0f1c9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* I choose the maximum accuracy from each gamma and C values combinations.","metadata":{"id":"K9F6UCOs041s"}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nAccurs_svm_gC_best = [.259, 0.3615, 0.476, 0.465, 0.4665]\nCs = [0.01, 0.1, 1, 10, 100]\nplt.plot([0.01, 0.1, 1, 10, 100],Accurs_svm_gC_best,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\n\nplt.title('Accuracy vs. C Value')\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.xscale('log')\n\nprint(\"Maximum Accuracy:-\",max(Accurs_svm_gC_best),\"at C =\",Cs[Accurs_svm_gC_best.index(max(Accurs_svm_gC_best))])","metadata":{"id":"dMzr6BriLMQ-","outputId":"af74b4a9-676d-44e4-85a2-fc7fbbe34685"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confuion matrix","metadata":{"id":"RaqoT5Zd0hQZ"}},{"cell_type":"markdown","source":"### The model with best accuracy  ","metadata":{"id":"QR9cpRuU0lnN"}},{"cell_type":"code","source":"lin_clf = SVC(gamma=.01, C = 1)\nlin_clf.fit(d2_train_dataset, np.ravel(y_train_s,order='C'))\npre_svm_best = lin_clf.predict(d2_test_dataset)\naccr_svm_best = lin_clf.score(d2_test_dataset, np.ravel(y_test_s,order='C'))","metadata":{"id":"vyXhwQNG0hEC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr_svm_best","metadata":{"id":"N_xXruka3lCb","outputId":"0e749ac6-198b-4a57-9325-70d9bfa3a4a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix = metrics.confusion_matrix(y_test_s, pre_svm_best)\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(y_test_s))\n\ncm_display.plot()\nplt.title('Confusion Matrix for cifar10')\nplt.show()","metadata":{"id":"OnQo3mTZ3mhn","outputId":"4bc9f5b1-eeca-4672-a92d-ffa9c92ecd8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have 200 value for each class, The confuison matrix says that the model is able to predict the class eight better than other classes as it manages to predict 129 images correctly, and managed to predict 125 imagess of 9 class correctly.\n\n* But the other classes correct predict was less than the half of the total values in a single class.\n\n* The class that the model was confused with the most is class 3, as it only manages to predict 66 images.","metadata":{"id":"XM1huwi1V3lg"}},{"cell_type":"markdown","source":"## Conclusion\nIn this part we used SVM from SCikit-learn, which can handel multiclass classification,\nI make different experience:\n* Build a baseline modsel using LinearSVC() function, and its output was 28.15%\n* Then I used different values for cost parameter and explore the effect of it on the model accuracy, and find out:\n  --> The best accuracy is 37.85% when C = 100, and show the plot of accuracy vs. C-parameter.\n  --> I plot the results as an accuracy vs. C-parameter curve on a logarithmic scale\n\n* In another experiment, I add L1 regularization, and plot the find out that the best accuracy reached 37.05%, which shows no improvement.\n  --> again, I plot the results as an accuracy vs. C-parameter curve on a logarithmic scale.\n\n* In the final experiment, I used different combination of cost and gamma,\n  --> I draw the confuision matrix for the prediction of model that has the best performance.\n  and plot the results as an accuracy vs. C-parameter curve on a logarithmic scale.\n  --> find out that the best combination is when gamma = .01 and C= 1","metadata":{"id":"_8pDpi6zFn_C"}},{"cell_type":"markdown","source":"## Part 4. Write a Conclusion\n\nWrite a conclusion comparing the results from each part of the assignment. Comment on the suitability of each method for this task.","metadata":{"id":"pTyPwN-YGD-K"}},{"cell_type":"markdown","source":"K-NN:\n1. With random K = 3, the KNN model with euclidean distance metric, the accuracy = 26.15%.\n2. After train the model with different values of K, the best accuracy was 29.25% with k = 17.\n3. The class the model was able to detect the best was class 8.\n\nLogistic Regression:\n1. The argmax of all 10 models computed from the OvR has an accuracy = 28.45, which is worse than KNN.\n2. The class the model was able to detect the best was class 6.\n\nSVM:\n1. The baseline model (LinearSVC()) has accuracy = 28.15 which is worse than the previous models.\n2. When training differnt values of C, the best accuracy was = 37.58, at C = 100, which is better than the previous models.\n3. By adding L1 regularization, and train with different values of C, the best accuracy was = 37.05, at C = .1, which is worse than before using it.\n4. When exploring the differnt values of Cost and gamma, the best accuracy was = 47.6, at C = 1 and gamma = .01, which is worse than before using it.\n\n\n--> I also add comments after each part above.","metadata":{"id":"wGr1-pzmj4se"}},{"cell_type":"markdown","source":"<!-- Write your conclusion here -->","metadata":{"id":"NZyFUQWSGD-K"}}]}